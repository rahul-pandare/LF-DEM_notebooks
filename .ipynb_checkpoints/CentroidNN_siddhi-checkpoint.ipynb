{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c61be29",
   "metadata": {},
   "source": [
    "### the dataset is split into train and test folders with 481 train images and 128 test images \n",
    "### the annotations are 68 landmark points which have the x,y coordinates in two columns . The name of the csv file and the images are the same. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068b2a5",
   "metadata": {},
   "source": [
    "### we first find the centroid of each vertebrae and use that as the labels for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b107e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from __future__ import print_function, division\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torchvision import io as tio\n",
    "from torch.autograd import Variable\n",
    "from torch import flatten\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77beac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Preprocessing Helper functions\n",
    "\n",
    "def gamma_correction(img, gamma=1.0):\n",
    "    gamma_corrected = np.array(255*(img / 255) ** gamma, dtype = 'uint8')\n",
    "    #gamma_corrected = np.array(img ** gamma)\n",
    "    return gamma_corrected\n",
    "\n",
    "\n",
    "def select_gamma(img):\n",
    "    av = np.average(img)\n",
    "    gamma = av/30\n",
    "    return gamma\n",
    "\n",
    "def apply_clahe(img, limit):\n",
    "    clahe = cv2.createCLAHE(clipLimit=limit)\n",
    "    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_GRAY2RGB)\n",
    "    #img = cv2.cvtColor(cv2.COLOR_GRAY2RGB)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    img = clahe.apply(img)\n",
    "    return img\n",
    "\n",
    "def normalize(img):\n",
    "    return (img-np.min(img))/(np.max(img)-np.min(img))\n",
    "\n",
    "def preprocess_img(img, num_sections):\n",
    "    sections = []\n",
    "    gammas = []\n",
    "    gamma_corrected_xray = img.copy()\n",
    "\n",
    "    for i in range(num_sections):\n",
    "        if i == 0:\n",
    "            sections.append(gamma_corrected_xray[:int(gamma_corrected_xray.shape[0]/num_sections), :])\n",
    "        elif i == num_sections-1:\n",
    "            sections.append(gamma_corrected_xray[int(gamma_corrected_xray.shape[0]*(i)/num_sections+1):, :])\n",
    "        else:\n",
    "            sections.append(gamma_corrected_xray[int(gamma_corrected_xray.shape[0]*(i)/num_sections):int(gamma_corrected_xray.shape[0]*(i+1)/num_sections), :])\n",
    "\n",
    "\n",
    "        gammas.append(select_gamma(sections[i]))\n",
    "        sections[i] = gamma_correction(sections[i], gammas[i])\n",
    "\n",
    "        sections[i] = 255*normalize(sections[i])\n",
    "        #sections[i] = normalize(sections[i])\n",
    "\n",
    "\n",
    "    for i in range(num_sections):\n",
    "        gamma_weighted = 255*(gammas[i]/max(gammas))\n",
    "        #gamma_weighted = (gammas[i]/max(gammas))\n",
    "\n",
    "        if i == 0:\n",
    "            gamma_corrected_xray[:int(gamma_corrected_xray.shape[0]/num_sections), :] = sections[i] #255*normalize(((1/gamma_weighted)*img[:int(gamma_corrected_xray.shape[0]/num_sections), :] + gamma_weighted*sections[i])/2)\n",
    "        elif i == num_sections-1:\n",
    "            gamma_corrected_xray[int(gamma_corrected_xray.shape[0]*(i)/num_sections+1):, :] = sections[i] #255*normalize(((1/gamma_weighted)*img[int(gamma_corrected_xray.shape[0]*(i)/num_sections+1):, :] +gamma_weighted*sections[i])/2)\n",
    "        else:\n",
    "            gamma_corrected_xray[int(gamma_corrected_xray.shape[0]*(i)/num_sections):int(gamma_corrected_xray.shape[0]*(i+1)/num_sections), :] = sections[i] #255*normalize(((1/gamma_weighted)*img[int(gamma_corrected_xray.shape[0]*(i)/num_sections):int(gamma_corrected_xray.shape[0]*(i+1)/num_sections), :] + gamma_weighted*sections[i])/2)\n",
    "    return gamma_corrected_xray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86aa8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size. This scales the image AND the corresponding landmarks \n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        self.num_sections = 100\n",
    "\n",
    "    def __call__(self, sample):\n",
    "       \n",
    "        image_temp, landmarks = sample['image'], (sample['landmarks'])\n",
    "        image = preprocess_img(image_temp, self.num_sections)\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img= transform.resize(image, (new_h, new_w))\n",
    "        \n",
    "        \n",
    "        for i in range(len(landmarks)):\n",
    "            if i % 2 == 0:\n",
    "               \n",
    "               # landmarks[i] = landmarks[i] * (new_w / w) \n",
    "                landmarks[i] = float(landmarks[i]) * (1 / w) \n",
    "                \n",
    "                \n",
    "            else: \n",
    "               # landmarks[i] = landmarks[i] * new_h / h\n",
    "                \n",
    "                landmarks[i] = float(landmarks[i])  * ( 1 / h )\n",
    "               \n",
    "            \n",
    "       \n",
    "    # here we scale the landmarks between 0 to 1 that is the largest coordinate becomes 1. \n",
    "    \n",
    "    \n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        return {'image': torch.from_numpy(image)[None].float(), 'landmarks': torch.from_numpy(np.float32(landmarks))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a686a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroid(x1, y1, x2, y2, x3, y3, x4, y4):\n",
    "    \"\"\"\n",
    "    This function takes in the (x,y) coordinates of the four corners of a quadrilateral and returns the (x,y) coordinates of its centroid.\n",
    "    \"\"\"\n",
    "    x_centroid = float((x1 + x2 + x3 + x4) / 4)\n",
    "    y_centroid = float((y1 + y2 + y3 + y4) / 4)\n",
    "    return x_centroid, y_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a031c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_centroids(csv_file):\n",
    "\n",
    "    \n",
    "    landmark_frame = pd.DataFrame(pd.read_csv(csv_file, header=None))\n",
    "    landmark_centroid = list()\n",
    "\n",
    "    index = 0 \n",
    "    for rows in range(0,int(len(landmark_frame)/4)):\n",
    "        x1 = (landmark_frame[0][index])\n",
    "        x2 = (landmark_frame[0][index + 1])\n",
    "        x3 = (landmark_frame[0][index + 2])\n",
    "        x4 = (landmark_frame[0][index + 3])\n",
    "        y1 = (landmark_frame[1][index])\n",
    "        y2 =  (landmark_frame[1][index + 1])\n",
    "        y3 = (landmark_frame[1][index + 2])\n",
    "        y4 = (landmark_frame[1][index + 3])\n",
    "        index += 4\n",
    "        x_centroid,  y_centroid = find_centroid(x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "        landmark_centroid.append( x_centroid)\n",
    "        landmark_centroid.append( y_centroid)\n",
    "\n",
    "    return landmark_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8cb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks, h, w):\n",
    "#     points = list()\n",
    "#     index = 0\n",
    "#     for i in range(0,int(len(landmarks)/2)):\n",
    "#         x_point = landmarks[index]\n",
    "#         y_point = landmarks[index+1]\n",
    "#         points.append([x_point, y_point])\n",
    "#         index += 2\n",
    "#     points = np.array(points)\n",
    "#     plt.imshow(image,cmap = \"gray\")\n",
    "#     plt.scatter(points[:, 0], points[:, 1], marker=\"o\", color=\"red\", s=20)\n",
    "#     plt.pause(0.001)\n",
    "    zeroesjpg = image.cpu().detach().numpy().copy()\n",
    "    landmarks = landmarks.cpu().detach().numpy().copy()\n",
    "    index = 0\n",
    "    points = list()\n",
    "    for i in range(0,int(len(landmarks)/2)):\n",
    "        x_point = landmarks[index]*w\n",
    "       \n",
    "        y_point = landmarks[index+1]*h\n",
    "        \n",
    "        points.append([x_point, y_point])\n",
    "        index += 2\n",
    "   \n",
    "    plt.figure(figsize=(10,6))\n",
    "    points = np.array(points)\n",
    "    plt.imshow(image,cmap = \"gray\")\n",
    "    plt.scatter(points[:, 0], points[:, 1], marker=\"o\", color=\"red\", s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73324ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd1057d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n",
      "481\n",
      "F:\\Georgia Tech\\1st year\\MIP\\Project Codes\\Dataset\\data\\training\\sunhl-1th-02-Jan-2017-162 A AP.jpg\n"
     ]
    }
   ],
   "source": [
    "path_labels = r\"F:\\Georgia Tech\\1st year\\MIP\\Project Codes\\Dataset\\Points\\training\"\n",
    "labels_training = list()\n",
    "for file in os.listdir(path_labels):\n",
    "    temp  = write_centroids(os.path.join(path_labels,file))\n",
    "    labels_training.append(temp)\n",
    "\n",
    "print(len(labels_training))\n",
    "\n",
    "path_img =  r\"F:\\Georgia Tech\\1st year\\MIP\\Project Codes\\Dataset\\data\\training\"\n",
    "image_dir_training = list()\n",
    "\n",
    "for file in os.listdir(path_img):\n",
    "    temp = os.path.join(path_img,file)\n",
    "    image_dir_training.append(temp)\n",
    "    \n",
    "print(len(image_dir_training))\n",
    "print(image_dir_training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8834fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "F:\\Georgia Tech\\1st year\\MIP\\Project Codes\\Dataset\\data\\test\\sunhl-1th-01-Mar-2017-310 a ap.jpg\n"
     ]
    }
   ],
   "source": [
    "path_labels = r\"F:\\Georgia Tech\\1st year\\MIP\\Project Codes\\Dataset\\Points\\testing\"\n",
    "labels_testing = list()\n",
    "for file in os.listdir(path_labels):\n",
    "    temp  = write_centroids(os.path.join(path_labels,file))\n",
    "    labels_testing.append(temp)\n",
    "\n",
    "print(len(labels_testing))\n",
    "\n",
    "path_img =  r\"F:\\Georgia Tech\\1st year\\MIP\\Project Codes\\Dataset\\data\\test\"\n",
    "image_dir_testing = list()\n",
    "\n",
    "for file in os.listdir(path_img):\n",
    "    temp = os.path.join(path_img,file)\n",
    "    image_dir_testing.append(temp)\n",
    "    \n",
    "print(len(image_dir_testing))\n",
    "print(image_dir_testing[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2edeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpineLandmarksDataset(Dataset):\n",
    "\n",
    "    def __init__(self,image_dir,labels, transform=None):\n",
    "\n",
    "        self.landmarks = labels\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks) \n",
    "        \n",
    "    def __getitem__(self, idx): \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.image_dir[idx]\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = np.array(self.landmarks[idx])\n",
    "        \n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff4a28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # dataloader are nor subscriptable only spine_dataset is subscriptable\n",
    "# height = 256\n",
    "# width = 128\n",
    "# spine_dataset_training = SpineLandmarksDataset(image_dir_training, labels_training, transform=transforms.Compose([Rescale((height,width)),ToTensor()]))\n",
    "\n",
    "# spine_dataset_training.__getitem__(15)\n",
    "# #show_landmarks(**spine_dataset_training[15], h = height, w = width)\n",
    "\n",
    "\n",
    "# spine_dataset_testing = SpineLandmarksDataset(image_dir_testing, labels_testing, transform=transforms.Compose([Rescale((height,width)),ToTensor()]))\n",
    "\n",
    "# spine_dataset_testing.__getitem__(15)\n",
    "# #show_landmarks(**spine_dataset_testing[15], h = height, w = width)\n",
    "\n",
    "# i = spine_dataset_testing.__getitem__(15)\n",
    "# print(np.shape(i[\"image\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a143588d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#(Classifier for the dataset.\n",
    "#(Classifier for the dataset.\n",
    "class SpineNetClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, classes = 34):\n",
    "        super(SpineNetClassifier, self).__init__()\n",
    "\n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "\n",
    "        #Input shape= (16,1,256,128) (batch_size, channels, height, width)\n",
    "\n",
    "        self.Conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=7,stride=1,padding=3)\n",
    "        #(16,32,256,128)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #(16,32,256,128)\n",
    "        self.MaxPool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        #(16,32,128,64)\n",
    "\n",
    "        self.Conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride = 1, padding =1)\n",
    "        #(16,64,128,64)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #(16,64,128,64)\n",
    "        self.MaxPool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        #(16,64,64,32)\n",
    "\n",
    "        self.Conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride = 1, padding =1)\n",
    "        #(16,128,64,32)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #(16,128,64,32)\n",
    "        self.MaxPool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        #(16,128,32,16)\n",
    "\n",
    "        self.Conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride = 1, padding =2)\n",
    "        #(16,256,32,16)\n",
    "        self.relu4=nn.ReLU()\n",
    "         #(16,256,32,16)\n",
    "        self.MaxPool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        #(16,256,16,8)\n",
    "\n",
    "        self.FC1 = nn.Linear(in_features= 256* 16* 8, out_features=512)\n",
    "        self.relu5=nn.ReLU()\n",
    "        self.FC2 = nn.Linear(in_features=512, out_features=512)\n",
    "        self.relu6=nn.ReLU()\n",
    "        self.FC3 = nn.Linear(in_features=512, out_features=classes)\n",
    "        # self.FC4 =  nn.Linear(in_features=512, out_features=classes)\n",
    "\n",
    "\n",
    " \n",
    "    def forward(self,input):\n",
    "        output = self.Conv1(input)\n",
    "        output = self.relu1(output)\n",
    "        output = self.MaxPool1(output)\n",
    "\n",
    "        output = self.Conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.MaxPool2(output)\n",
    "\n",
    "        output = self.Conv3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.MaxPool3(output)\n",
    "\n",
    "        output = self.Conv4(output)\n",
    "        output = self.relu4(output)\n",
    "        output = self.MaxPool4(output)\n",
    "\n",
    "        output = flatten(output,1)\n",
    "\n",
    "        output = self.FC1(output)\n",
    "        output = self.relu5(output)\n",
    "        output = self.FC2(output)\n",
    "        output = self.relu6(output)\n",
    "        output = self.FC3(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7cc9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the classifier.\n",
    "#Classes is 34 Outputs, centroid points for each vertebrae.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe9065c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\siddhi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 256, 128]           1,600\n",
      "              ReLU-2         [-1, 32, 256, 128]               0\n",
      "         MaxPool2d-3          [-1, 32, 128, 64]               0\n",
      "            Conv2d-4          [-1, 64, 128, 64]          18,496\n",
      "              ReLU-5          [-1, 64, 128, 64]               0\n",
      "         MaxPool2d-6           [-1, 64, 64, 32]               0\n",
      "            Conv2d-7          [-1, 128, 64, 32]          73,856\n",
      "              ReLU-8          [-1, 128, 64, 32]               0\n",
      "         MaxPool2d-9          [-1, 128, 32, 16]               0\n",
      "           Conv2d-10          [-1, 256, 32, 16]         819,456\n",
      "             ReLU-11          [-1, 256, 32, 16]               0\n",
      "        MaxPool2d-12           [-1, 256, 16, 8]               0\n",
      "           Linear-13                  [-1, 512]      16,777,728\n",
      "             ReLU-14                  [-1, 512]               0\n",
      "           Linear-15                  [-1, 512]         262,656\n",
      "             ReLU-16                  [-1, 512]               0\n",
      "           Linear-17                   [-1, 34]          17,442\n",
      "================================================================\n",
      "Total params: 17,971,234\n",
      "Trainable params: 17,971,234\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 33.77\n",
      "Params size (MB): 68.55\n",
      "Estimated Total Size (MB): 102.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "width = 128\n",
    "height = 256\n",
    "model = SpineNetClassifier(classes=34).to(device)\n",
    "summary(model, input_size=(1,height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d60dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38aa8b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "spine_dataset_training = SpineLandmarksDataset(image_dir_training, labels_training, transform=transforms.Compose([Rescale((height,width)),ToTensor()]))\n",
    "train_dataloader = DataLoader(spine_dataset_training, batch_size, shuffle=True, num_workers = 0)\n",
    "print(train_dataloader.__len__())\n",
    "\n",
    "spine_dataset_testing = SpineLandmarksDataset(image_dir_testing, labels_testing, transform=transforms.Compose([Rescale((height,width)),ToTensor()]))\n",
    "test_dataloader = DataLoader(spine_dataset_testing, batch_size, shuffle=True, num_workers = 0)\n",
    "print(test_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f3d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_step(model, sample, optimizer):\n",
    "#     optimizer.zero_grad()\n",
    "#     for k,v in sample.items():\n",
    "#         data[k] = v.to(device)\n",
    "#     loss = model (**data)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850334e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch(model,data_loader,optimizer):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch, sample in enumerate(data_loader):\n",
    "#         loss = train_one_step(model,sample,optimizer)\n",
    "       \n",
    "#         total_loss += loss \n",
    "        \n",
    "#     return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35d2b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_one_step(model, sample,):\n",
    "#     for k,v in sample.items():\n",
    "#         data[k] = v.to(device)\n",
    "#     loss = model (**data)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3312e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_one_epoch(model,data_loader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     for batch, sample in enumerate(data_loader):\n",
    "#         with torch.no_grad():\n",
    "#             loss = test_one_step(model,sample)\n",
    "\n",
    "#         total_loss += loss \n",
    "        \n",
    "#     return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ace4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "#     size = dataloader.__len__()\n",
    "#     totalTrainLoss = 0 \n",
    "#     for batch, sample in enumerate(dataloader):\n",
    "#         image = sample[\"image\"]\n",
    "#         landmarks = sample[\"landmarks\"]\n",
    "#         image, landmarks = image.to(device), landmarks.to(device)\n",
    "#         pred = model(image)\n",
    "#         loss = loss_fn(pred,landmarks)\n",
    "        \n",
    "        \n",
    "#         #backpropogation \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         totalTrainLoss += loss.item()\n",
    "        \n",
    "#     avgTrainLoss = totalTrainLoss / size \n",
    "    \n",
    "# def test_loop (dataloader, model, loss_fn):\n",
    "#     size = dataloader.__len__()\n",
    "#     test_loss = 0 \n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch, sample in enumerate(dataloader):\n",
    "#     # Compute prediction and loss\n",
    "#             image=sample[\"image\"]\n",
    "#             landmarks=sample[\"landmarks\"]\n",
    "#             image, landmarks = image.to(device), landmarks.to(device)\n",
    "#             pred = model(image)\n",
    "#             test_loss += loss_fn(pred, landmarks).item()\n",
    "\n",
    "#     avgTestLoss = test_loss/size\n",
    "#     Hist[\"test_loss\"].append(avgTestLoss)\n",
    "  \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49d47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),lr=0.0005) #learning rate\n",
    "loss_fn = nn.MSELoss()\n",
    "Hist = {\"train_loss\": [],\"test_loss\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b52f9f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIDDHI\\AppData\\Local\\Temp/ipykernel_20228/612139641.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (img-np.min(img))/(np.max(img)-np.min(img))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 2.00 GiB total capacity; 1.17 GiB already allocated; 0 bytes free; 1.17 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20228/3451242646.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m#test_loss+= loss.cpu().data*images.size(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\siddhi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20228/3404769098.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\siddhi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\siddhi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\siddhi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 2.00 GiB total capacity; 1.17 GiB already allocated; 0 bytes free; 1.17 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#start training and at save the best model for evaluation\n",
    "\n",
    "train_loss_epoch = list()  # using these list to see the performance on every epoch\n",
    "test_loss_epoch = list()\n",
    "num_epochs = 10\n",
    "size_train = train_dataloader.__len__()\n",
    "\n",
    "for epoch in range(num_epochs): # for every epoch load the entire training data set calculate loss and append per epoch loss\n",
    "    \n",
    "    #model training on training dataset\n",
    "    model.train()\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for batch, sample  in enumerate(train_dataloader):\n",
    "        \n",
    "        images = sample[\"image\"]\n",
    "        landmarks = sample[\"landmarks\"]\n",
    "        images, landmarks = images.to(device), landmarks.to(device)\n",
    "#             images=Variable(images.cuda())\n",
    "#             labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss=loss_fn(outputs,landmarks) #calculate loss\n",
    "        loss.backward()  #back prop\n",
    "        optimizer.step() \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #train_loss+= loss.cpu().data*images.size(0) #sum the training loss for each image in the batch\n",
    "    \n",
    "        \n",
    "        \n",
    "    train_loss=train_loss/ size_train #average over all images\n",
    "    train_loss_epoch.append(train_loss) #append the epoch loss\n",
    "    \n",
    "    # Now evaluating on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    \n",
    "    for batch, sample  in enumerate(test_dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = sample[\"image\"]\n",
    "            landmarks = sample[\"landmarks\"]\n",
    "#             images=Variable(images.cuda())\n",
    "#             labels=Variable(labels.cuda())\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_fn(outputs,landmarks)\n",
    "        #test_loss+= loss.cpu().data*images.size(0)\n",
    "        \n",
    "        test_loss += loss_fn(outputs,landmarks)\n",
    "        \n",
    "\n",
    "    test_loss = test_loss/size_test\n",
    "    test_loss_epoch.append(test_loss) #append the epoch loss\n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Training Loss: '+str(np.float64(train_loss))+' Testing Loss: '+str(np.float64(test_loss)))\n",
    "    \n",
    " \n",
    " \n",
    "torch.save(model.state_dict(),'10_epochs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")\n",
    "# torch.save(model.state_dict(),'10_epochs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a07082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01df3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = copy.deepcopy(model) # create a deep copy so that all state dictionaries are also saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbc461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a3592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
