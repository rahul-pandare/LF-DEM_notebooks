{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbe891e",
   "metadata": {},
   "source": [
    "## Analysis of simulation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3a3e7",
   "metadata": {},
   "source": [
    "### This notebooks contains scripts:\n",
    "1. to read and pickle the rheology data, F_rig and z_net\n",
    "2. load the pickle file\n",
    "3. make plots using the bidi function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c89541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import numpy             as     np\n",
    "import pandas            as     pd\n",
    "import scipy.optimize    as     opt\n",
    "import matplotlib.pyplot as     plt\n",
    "from   matplotlib        import font_manager\n",
    "from   fractions         import Fraction\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='mkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73238d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f092b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "script to read and pickle data.\n",
    "Here we read, concatenate and pickle the simulation data from data_*.dat file, F_rig and z_net\n",
    "\n",
    "'''\n",
    "\n",
    "# Reading directory:\n",
    "\n",
    "TopDir      = \"/media/Linux_1TB/simulations/\" #(samsung ssd)\n",
    "\n",
    "pkldumpdir  =\"/media/Linux_1TB/Dropbox (City College)/CUNY/Research/Bidisperse Project/analysis/\" #office linux\n",
    "\n",
    "NP          = [1000]\n",
    "\n",
    "run         = {500:8,1000:4,2000:2,4000:1}\n",
    "\n",
    "phi         = [0.75]\n",
    "\n",
    "ar          = [1.4]\n",
    "\n",
    "# data_file = 'data_contact_1.pkl'\n",
    "\n",
    "# if os.path.exists(pkldumpdir+data_file):\n",
    "#     with open(pkldumpdir+data_file, 'rb') as file:\n",
    "#         datasets_dict = pickle.load(file)\n",
    "#     print(f\"\\nData loaded from existing '{data_file}' pickle file\")\n",
    "\n",
    "\n",
    "# files to read\n",
    "ranSeedFile = 'random_seed.dat'\n",
    "intFile = 'int_random_seed_params_stress100r_shear.dat'\n",
    "\n",
    "\n",
    "# initializing the dictionary\n",
    "contactList=[]                    \n",
    "#datasets_dict = {key: None for key in datasets}\n",
    "\n",
    "for i in range(len(NP)):\n",
    "    for j in range(len(phi)):\n",
    "        phir = '{:.3f}'.format(phi[j]) if len(str(phi[j]).split('.')[1])>2 else '{:.2f}'.format(phi[j])\n",
    "        for k in range(len(ar)):\n",
    "            dataname=TopDir+'NP_'+str(NP[i])+'/phi_'+phir+'/ar_'+str(ar[k])+'/Vr_0.5'\n",
    "            if os.path.exists(dataname):\n",
    "                temp=[]\n",
    "                for l in range (run[NP[i]]):\n",
    "                    with open(f'{dataname}/run_{l+1}/{ranSeedFile}', 'r') as file:\n",
    "                        particleSize = np.loadtxt(file,usecols=(3,))\n",
    "                    \n",
    "                    with open(f'{dataname}/run_{l+1}/{intFile}', 'r') as file:\n",
    "                        particleSize = np.loadtxt(file,usecols=(3,))\n",
    "                    \n",
    "                   \n",
    "                        dat_run=0\n",
    "                        with open(f'{dataname}/run_{l+1}/{file_name}', 'r') as file:\n",
    "                            dat_run = np.loadtxt(file)\n",
    "\n",
    "                    temp.append(dat_run)\n",
    "                datasets_dict['NP_'+str(NP[i])+'_phi_'+phir+'_ar_'+str(ar[k])]=temp\n",
    "\n",
    "with open(pkldumpdir+data_file, 'wb') as file:\n",
    "    pickle.dump(datasets_dict, file)\n",
    "with open(pkldumpdir+data_file, 'rb') as file:\n",
    "    datasets_dict = pickle.load(file)\n",
    "\n",
    "print(\"\\nData read and loaded \\n\")\n",
    "    \n",
    "    \n",
    "m='''\n",
    "Data structure - \n",
    "datasets_dict contains the data of all parameters and all samples\n",
    "\n",
    "datasets_dict: {'key1':[run1, run2, run3, run4], 'key2': [run1, run2, run3, run4],.....}\n",
    "\n",
    "eg: 'key1': 'NP_1000_phi_0.70_ar_1.0'\n",
    "    run1.shape = (5001 (samples),35 (parameters))\n",
    "\n",
    "'''what "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad4f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranSeedFile = 'random_seed.dat'\n",
    "with open(f'/media/Linux_1TB/simulations/NP_1000/phi_0.75/ar_1.4/Vr_0.5/run_1/{ranSeedFile}', 'r') as file:\n",
    "    particleIndex = np.loadtxt(file,usecols=(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b20548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particleIndex[819]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1279281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this works\n",
    "\n",
    "ranSeedFile = 'random_seed.dat'\n",
    "intFile = 'int_random_seed_params_stress100r_shear.dat'\n",
    "\n",
    "hashCounter=0\n",
    "contactDat =[]\n",
    "\n",
    "with open(f'/media/Linux_1TB/simulations/NP_1000/phi_0.75/ar_1.4/Vr_0.5/run_1/{intFile}', 'r') as file:\n",
    "    intFileDat = file.readlines()[27:]\n",
    "    temp=[]\n",
    "    for i in range(len(intFileDat)):\n",
    "        if intFileDat[i].split()[0] != '#':\n",
    "            temp.append(intFileDat[i].split())\n",
    "        else:\n",
    "            hashCounter += 1\n",
    "            if hashCounter == 7:\n",
    "                contactDat.append(np.array(temp))\n",
    "                hashCounter=0\n",
    "                temp=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b057c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"#\" in intFileDat[9660-26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d16d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any('#' in array for array in contactDat):\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f160ae61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4635590)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(intFileDat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb911d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname=TopDir+'NP_'+str(NP[i])+'/phi_'+phir+'/ar_'+str(ar[k])+'/Vr_0.5'\n",
    "for m, file_name in enumerate(file_names):\n",
    "    sum_variables[m]=0\n",
    "    with open(f'{dataname}/run_{0+1}/{file_name}', 'r') as file:\n",
    "        sum_variables[m] = np.loadtxt(file)\n",
    "        #sum_variables[1]=np.expand_dims(sum_variables[1], axis=1)       \n",
    "        dat_run=np.concatenate(sum_variables,axis=1)\n",
    "        temp.append(dat_run)\n",
    "    datasets_dict['NP_'+str(NP[i])+'_phi_'+phir+'_ar_'+str(ar[k])]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1279ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e099a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(datasets_dict['NP_1000_phi_0.75_ar_2.0'][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd8442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c682fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
